import streamlit as st
import joblib
import re
# Load Saved ML Model & Base Word List
toxicity_model = joblib.load("model/toxic_model.pkl")
base_bad_words = joblib.load("model/final_bad_words.pkl")
# Massive English + Telugu Offensive Dictionary
english_offensive_words = [
    "fuck","fucking","fucker","fucked","mf","motherfucker","bitch","bitches",
    "bich","biatch","slut","whore","hoe","cunt","pussy","dick","cock","prick",
    "asshole","asshat","asswipe","dumbass","jackass","shit","shitty","bullshit",
    "crap","bastard","jerk","retard","fucktard","douche","douchebag","moron",
    "idiot","stupid","dumb","loser","scumbag","piss off","pisshead","pissbrain",
    "pissface","twat","skank","tramp","pig","donkey","airhead","brainless",
    "jerkoff","cocksucker","dickhead","wtf","hell","damn","f@ck","a$$hole",
    "sl*t","wh0re","c*nt","p*ssy","di*k","co*k","fuk","fck","fucc","fvk","fukr",
    "harami","gandu","kutti","kutta","rand","randi"
]
telugu_offensive_words = [
    "‡∞µ‡±Ü‡∞ß‡∞µ","‡∞Æ‡±Ç‡∞∞‡±ç‡∞ñ‡±Å‡∞°‡±Å","‡∞¶‡∞¶‡±ç‡∞¶‡∞Æ‡±ç‡∞Æ","‡∞ö‡±Ü‡∞§‡±ç‡∞§‡±ã‡∞°‡±Å","‡∞®‡∞æ‡∞Ø‡∞æ‡∞≤‡∞æ","‡∞¶‡±Å‡∞∞‡±ç‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡±Å‡∞°‡±Å",
    "‡∞®‡±Ä‡∞ö‡±Å‡∞°‡±Å","‡∞™‡∞ø‡∞ö‡±ç‡∞ö‡±ã‡∞°‡±Å","‡∞¨‡±Å‡∞¶‡±ç‡∞ß‡∞ø‡∞≤‡±á‡∞®‡∞ø","‡∞™‡∞ø‡∞ö‡±ç‡∞ö‡∞ø","‡∞™‡∞ø‡∞ö‡±ç‡∞ö‡∞ø‡∞µ‡∞æ‡∞°‡±Å","‡∞ö‡±Ü‡∞§‡±ç‡∞§",
    "‡∞≤‡∞Ç‡∞ú","‡∞∞‡∞æ‡∞Ç‡∞°‡±Ä","‡∞¶‡±Ü‡∞Ç‡∞ó‡±Å","‡∞¶‡±Ü‡∞Ç‡∞ó‡±Å ‡∞ï‡±ä‡∞°‡±Å‡∞ï‡±Å","‡∞¶‡±Ü‡∞Ç‡∞ó‡∞ø‡∞®‡±ã‡∞°‡±Å","‡∞¶‡±ä‡∞Ç‡∞ó","‡∞¶‡±ä‡∞Ç‡∞ó‡∞Æ‡±Ç‡∞§",
    "‡∞Æ‡±ã‡∞∏‡∞ó‡∞æ‡∞°‡±Å","‡∞Ö‡∞∏‡∞π‡±ç‡∞Ø‡∞Ç","‡∞¶‡∞∞‡∞ø‡∞¶‡±ç‡∞∞‡±Å‡∞°‡±Å","‡∞Æ‡±Ç‡∞§‡±ç‡∞∞‡∞™‡±Å ‡∞Æ‡±Å‡∞ñ‡∞Ç","‡∞§‡∞ø‡∞ï‡±ç‡∞ï","‡∞§‡∞ø‡∞ï‡±ç‡∞ï‡±ã‡∞°‡±Å",
    "‡∞™‡∞ø‡∞∞‡∞ø‡∞ï‡∞ø‡∞µ‡∞æ‡∞°‡±Å","‡∞µ‡∞æ‡∞ú","‡∞™‡∞Ç‡∞¶‡∞ø","‡∞™‡∞Ç‡∞¶‡∞ø‡∞ï‡±ä‡∞ï‡±ç‡∞ï","‡∞∞‡∞æ‡∞ï‡±ç‡∞∑‡∞∏‡±Å‡∞°‡±Å","‡∞Ö‡∞∏‡∞≠‡±ç‡∞Ø‡±Å‡∞°‡±Å","‡∞¨‡±Ç‡∞§‡±Å",
    "‡∞¨‡±Ç‡∞§‡±Å‡∞≤‡∞æ‡∞°‡∞ü‡∞Ç","‡∞¨‡±Ç‡∞§‡±Å ‡∞Æ‡∞æ‡∞ü‡∞≤‡±Å","‡∞¶‡±Å‡∞∑‡±ç‡∞ü‡±Å‡∞°‡±Å","‡∞∏‡∞ø‡∞ó‡±ç‡∞ó‡±Å‡∞≤‡±á‡∞®‡∞ø","‡∞®‡±ã‡∞∞‡±Å ‡∞∏‡∞ø‡∞ó‡±ç‡∞ó‡±Å‡∞≤‡±á‡∞®‡∞ø",
    "‡∞ó‡±Å‡∞£‡∞Ç ‡∞≤‡±á‡∞®‡∞ø","‡∞Ø‡∞µ‡±ç‡∞µ‡∞æ‡∞∞‡∞Ç","‡∞¶‡±Å‡∞∞‡±ç‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡∞Ç","‡∞¶‡∞∞‡∞ø‡∞¶‡±ç‡∞∞‡∞™‡±Å","‡∞∞‡∞æ‡∞ï‡±ç‡∞∑‡∞∏‡∞™‡±Å"
]
# Merge PKL + manual lists
final_offensive_word_list = set(base_bad_words)
final_offensive_word_list.update(english_offensive_words)
final_offensive_word_list.update(telugu_offensive_words)
final_offensive_word_list = list(final_offensive_word_list)
# Misspellings Normalize
COMMON_MISSPELLINGS = {
    "idoit": "idiot", "stupit": "stupid", "fuk": "fuck", "fck": "fuck",
    "bich": "bitch", "asshloe": "asshole"
}
def preprocess_comment(comment_text):
    comment_text = comment_text.lower()
    comment_text = re.sub(r"http\S+|@\w+|#\w+", "", comment_text)
    comment_text = re.sub(r"[^\u0C00-\u0C7Fa-zA-Z\s]", "", comment_text)
    comment_text = re.sub(r"\s+", " ", comment_text).strip()
    return comment_text

def normalize_slang_words(clean_text):
    words = clean_text.split()
    fixed = [COMMON_MISSPELLINGS.get(w, w) for w in words]
    return " ".join(fixed)
# Compile Regex Patterns (HIGH ACCURACY)
mask_patterns = []

for word in final_offensive_word_list:
    if isinstance(word, str) and len(word.strip()) > 1:
        pattern = re.compile(rf"\b{re.escape(word)}\b", flags=re.IGNORECASE)
        mask_patterns.append(pattern)
# Masking Function
def mask_offensive_words(original):
    masked = original
    for pattern in mask_patterns:
        masked = pattern.sub(lambda m: "*" * len(m.group()), masked)
    return masked
# Final Toxicity Pipeline
def detect_and_mask(text):
    cleaned = normalize_slang_words(preprocess_comment(text))
    label = toxicity_model.predict([cleaned])[0]
    score = toxicity_model.predict_proba([cleaned])[0][1]

    if label == 1:
        masked = mask_offensive_words(text)
        return "Toxic ‚ö†Ô∏è", round(score * 100, 2), masked
    else:
        return "Non-Toxic ‚úÖ", round(score * 100, 2), text
# Streamlit UI
st.set_page_config(page_title="Toxic Comment Detector", page_icon="üõ°Ô∏è")

st.title("üõ°Ô∏è Toxic Comment Detection & Masking System")
st.write("Enter a comment below to check if it is toxic and automatically mask offensive words.")

user_input = st.text_area("Enter your comment:")

if st.button("Check Toxicity"):
    if not user_input.strip():
        st.warning("Please enter a comment.")
    else:
        label, score, masked = detect_and_mask(user_input)
        st.subheader("Result:")
        st.write("Prediction:", label)
        st.write("Confidence Score:", str(score) + "%")
        st.success(masked)
